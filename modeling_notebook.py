# Urban Sustainability Predictor - Modelagem
# 04_modeling.ipynb

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import joblib
import warnings
warnings.filterwarnings('ignore')

print("ü§ñ INICIANDO MODELAGEM DE SUSTENTABILIDADE URBANA")
print("="*60)

# 1. CARREGAMENTO E PREPARA√á√ÉO DOS DADOS
print("üìÅ Carregando dados processados...")
df = pd.read_csv('data/processed/urban_sustainability_processed.csv')

# Features para modelagem
feature_columns = [
    'population', 'gdp_per_capita', 'renewable_energy_pct', 'air_quality_index',
    'public_transport_coverage', 'waste_management_score', 'green_space_pct',
    'education_index', 'healthcare_index', 'gini_coefficient', 'co2_emissions_per_capita'
]

X = df[feature_columns]
y = df['sustainability_index']

print(f"‚úÖ Dados carregados: {X.shape[0]} amostras, {X.shape[1]} features")
print(f"üéØ Target: {y.name}")

# 2. DIVIS√ÉO DOS DADOS
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

print(f"\nüìä Divis√£o dos dados:")
print(f"   Treino: {X_train.shape[0]} amostras")
print(f"   Valida√ß√£o: {X_val.shape[0]} amostras") 
print(f"   Teste: {X_test.shape[0]} amostras")

# 3. PADRONIZA√á√ÉO DAS FEATURES
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Features padronizadas com RobustScaler")

# 4. DEFINI√á√ÉO DOS MODELOS
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),
    'SVR': SVR(kernel='rbf', C=1.0)
}

# 5. TREINAMENTO E AVALIA√á√ÉO INICIAL
print(f"\nüèãÔ∏è TREINANDO MODELOS...")
print("="*60)

results = {}
predictions = {}

for name, model in models.items():
    print(f"üìà Treinando {name}...")
    
    # Treinar modelo
    if name in ['Linear Regression', 'Ridge', 'Lasso', 'SVR']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_val_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
    
    # Calcular m√©tricas
    mse = mean_squared_error(y_val, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_val, y_pred)
    r2 = r2_score(y_val, y_pred)
    
    results[name] = {
        'MSE': mse,
        'RMSE': rmse,
        'MAE': mae,
        'R¬≤': r2
    }
    
    predictions[name] = y_pred
    
    print(f"   RMSE: {rmse:.3f} | R¬≤: {r2:.3f}")

# 6. COMPARA√á√ÉO DOS MODELOS
print(f"\nüìä RESULTADOS COMPARATIVOS:")
print("="*60)
results_df = pd.DataFrame(results).T.round(3)
print(results_df.sort_values('R¬≤', ascending=False))

# Visualiza√ß√£o dos resultados
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# RMSE
results_df.sort_values('RMSE')['RMSE'].plot(kind='bar', ax=axes[0,0], color='lightcoral')
axes[0,0].set_title('RMSE por Modelo')
axes[0,0].set_ylabel('RMSE')

# R¬≤
results_df.sort_values('R¬≤', ascending=False)['R¬≤'].plot(kind='bar', ax=axes[0,1], color='lightblue')
axes[0,1].set_title('R¬≤ por Modelo')
axes[0,1].set_ylabel('R¬≤')

# MAE
results_df.sort_values('MAE')['MAE'].plot(kind='bar', ax=axes[1,0], color='lightgreen')
axes[1,0].set_title('MAE por Modelo')
axes[1,0].set_ylabel('MAE')

# Predi√ß√µes vs Real (melhor modelo)
best_model_name = results_df.sort_values('R¬≤', ascending=False).index[0]
best_predictions = predictions[best_model_name]

axes[1,1].scatter(y_val, best_predictions, alpha=0.6)
axes[1,1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
axes[1,1].set_xlabel('Valores Reais')
axes[1,1].set_ylabel('Valores Preditos')
axes[1,1].set_title(f'Predi√ß√µes vs Real - {best_model_name}')

plt.tight_layout()
plt.savefig('reports/figures/model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# 7. OTIMIZA√á√ÉO DO MELHOR MODELO
print(f"\nüîß OTIMIZANDO O MELHOR MODELO: {best_model_name}")
print("="*60)

if best_model_name == 'XGBoost':
    # Grid Search para XGBoost
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 4, 5, 6],
        'learning_rate': [0.01, 0.1, 0.2],
        'subsample': [0.8, 0.9, 1.0]
    }
    
    best_model = xgb.XGBRegressor(random_state=42)
    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    
    optimized_model = grid_search.best_estimator_
    print(f"üéØ Melhores par√¢metros: {grid_search.best_params_}")

elif best_model_name == 'Random Forest':
    # Grid Search para Random Forest
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    best_model = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    
    optimized_model = grid_search.best_estimator_
    print(f"üéØ Melhores par√¢metros: {grid_search.best_params_}")

else:
    optimized_model = models[best_model_name]

# 8. AVALIA√á√ÉO FINAL NO CONJUNTO DE TESTE
print(f"\nüß™ AVALIA√á√ÉO FINAL NO CONJUNTO DE TESTE")
print("="*60)

if best_model_name in ['Linear Regression', 'Ridge', 'Lasso', 'SVR']:
    final_predictions = optimized_model.predict(X_test_scaled)
else:
    final_predictions = optimized_model.predict(X_test)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
final_mae = mean_absolute_error(y_test, final_predictions)
final_r2 = r2_score(y_test, final_predictions)

print(f"üìä M√©tricas Finais:")
print(f"   RMSE: {final_rmse:.3f}")
print(f"   MAE: {final_mae:.3f}")
print(f"   R¬≤: {final_r2:.3f}")
print(f"   Acur√°cia: {(1 - final_mae/y_test.mean())*100:.1f}%")

# 9. IMPORT√ÇNCIA DAS FEATURES
if hasattr(optimized_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': optimized_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nüîç IMPORT√ÇNCIA DAS FEATURES:")
    print("="*60)
    for idx, row in feature_importance.iterrows():
        print(f"{row['feature']:<25}: {row['importance']:.3f}")
    
    # Visualiza√ß√£o da import√¢ncia
    plt.figure(figsize=(12, 8))
    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')
    plt.title('Import√¢ncia das Features - Modelo Final')
    plt.xlabel('Import√¢ncia')
    plt.tight_layout()
    plt.savefig('reports/figures/feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()

# 10. AN√ÅLISE DE RES√çDUOS
residuals = y_test - final_predictions

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Res√≠duos vs Predi√ß√µes
axes[0].scatter(final_predictions, residuals, alpha=0.6)
axes[0].axhline(y=0, color='r', linestyle='--')
axes[0].set_xlabel('Valores Preditos')
axes[0].set_ylabel('Res√≠duos')
axes[0].set_title('Res√≠duos vs Predi√ß√µes')

# QQ-plot dos res√≠duos
from scipy import stats
stats.probplot(residuals, dist="norm", plot=axes[1])
axes[1].set_title('Q-Q Plot dos Res√≠duos')

# Histograma dos res√≠duos
axes[2].hist(residuals, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
axes[2].set_xlabel('Res√≠duos')
axes[2].set_ylabel('Frequ√™ncia')
axes[2].set_title('Distribui√ß√£o dos Res√≠duos')

plt.tight_layout()
plt.savefig('reports/figures/residual_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# 11. SALVANDO O MODELO FINAL
print(f"\nüíæ SALVANDO MODELO E SCALER...")
joblib.dump(optimized_model, 'models/urban_sustainability_model.pkl')
joblib.dump(scaler, 'models/scaler.pkl')

# Salvando metadados do modelo
model_metadata = {
    'model_type': best_model_name,
    'features': feature_columns,
    'rmse': final_rmse,
    'mae': final_mae,
    'r2': final_r2,
    'accuracy_pct': (1 - final_mae/y_test.mean())*100
}

import json
with open('models/model_metadata.json', 'w') as f:
    json.dump(model_metadata, f, indent=2)

print("‚úÖ Modelo salvo em 'models/urban_sustainability_model.pkl'")
print("‚úÖ Scaler salvo em 'models/scaler.pkl'")
print("‚úÖ Metadados salvos em 'models/model_metadata.json'")

# 12. PREDI√á√ÉO DE EXEMPLO
print(f"\nüîÆ EXEMPLO DE PREDI√á√ÉO:")
print("="*60)

# Cidade exemplo
example_city = {
    'population': 500000,
    'gdp_per_capita': 45000,
    'renewable_energy_pct': 60,
    'air_quality_index': 45,
    'public_transport_coverage': 80,
    'waste_management_score': 75,
    'green_space_pct': 25,
    'education_index': 85,
    'healthcare_index': 80,
    'gini_coefficient': 0.35,
    'co2_emissions_per_capita': 8
}

example_df = pd.DataFrame([example_city])
if best_model_name in ['Linear Regression', 'Ridge', 'Lasso', 'SVR']:
    example_scaled = scaler.transform(example_df)
    prediction = optimized_model.predict(example_scaled)[0]
else:
    prediction = optimized_model.predict(example_df)[0]

print("Caracter√≠sticas da cidade exemplo:")
for key, value in example_city.items():
    print(f"   {key}: {value}")
print(f"\nüéØ √çndice de Sustentabilidade Predito: {prediction:.1f}/100")

print(f"\nüéâ MODELAGEM CONCLU√çDA!")
print(f"üèÜ Melhor modelo: {best_model_name}")
print(f"üìä Performance final: R¬≤ = {final_r2:.3f}, RMSE = {final_rmse:.2f}")
print(f"üìÅ Arquivos salvos em 'models/' e gr√°ficos em 'reports/figures/'")
